"""
Hybrid Wine Recommendation System - Part 1: Preprocessing & EDA

Project Overview:
This notebook conducts the full preprocessing pipeline for a hybrid wine
recommendation system. The core challenge is to align two disparate datasets:
`vivno_dataset.csv` (Catalog A, for training) and
`winemag-data-130k-v2.csv` (Catalog B, for inference).
We also load `created_wine_ratings.csv` (our synthetic user data).

The primary goal is to solve the "Feature Mismatch" problem, where 'winemag'
lacks critical features ('ABV', 'color') that exist in 'vivno'.
We solve this by creating a "Bridge" using the only common feature, 'variety',
to map standardized features from 'vivno' onto 'winemag'.

Key Libraries Used (Not Taught in Class):
------------------------------------------
- sklearn.feature_extraction.text.TfidfVectorizer:
    - Purpose: Converts a collection of raw text documents (like wine
      descriptions) into a matrix of TF-IDF features (numerical vectors).
      This allows machine learning models to "understand" text.
    - Parameters Used:
        - `stop_words='english'`: Removes common, non-informative English
          words (e.g., 'the', 'a', 'is').
        - `max_features=5000`: Keeps only the top 5000 most frequent
          terms to save memory and reduce noise.
        - `ngram_range=(1, 2)`: Includes both single words (1-gram, e.g.,
          "fruit") and two-word phrases (2-gram, e.g., "red fruit") as features.

- scipy.sparse.save_npz:
    - Purpose: Saves a SciPy sparse matrix (which TfidfVectorizer produces)
      to a file in `.npz` format. This is much more memory-efficient
      than saving a dense NumPy array, which would be too large.

- pickle:
    - Purpose: A standard Python library used to serialize and save
      Python objects to a file. We use `pickle.dump` to save our list of
      TF-IDF feature names (the 5000 words) to `tfidf_features.pkl`.
      This list is the "key" to understand what the columns in the
      'tfidf_matrix.npz' file mean.

Sources:
- Datasets: Kaggle (`vivno_dataset.csv`, `winemag-data-130k-v2.csv`)
- `created_wine_ratings.csv`: Generated by `create_wine_ratings.py`.
- Methodology: All preprocessing logic and "Bridge" concept were
  developed based on EDA findings. Library usage refers to official
  Scikit-learn, Pandas, and Surprise documentation.
"""

import pandas as pd
import numpy as np
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import save_npz
import pickle

# --- 1. Setup ---

# Ignore warning messages for cleaner output
warnings.filterwarnings("ignore")

# Set pandas option to display all columns in a DataFrame
pd.set_option("display.max_columns", None)

# Set matplotlib option to correctly display minus signs on plots
plt.rcParams["axes.unicode_minus"] = False

print("Libraries loaded.")

# --- 0. Data Loading ---
# Load the 3 core datasets for this project.
# w: vivno (Catalog A - Training Content)
# wm: winemag (Catalog B - Inference/Recommendation Target Content)
# ratings: created_wine_ratings (User Ratings Data)

# 1. vivno (Catalog A - Training Content)
# (Explanation: 'encoding="utf-16", sep=";"' is required because
# this file is not a standard comma-separated, utf-8 file.)
raw_vivno = pd.read_csv("vivno_dataset.csv", encoding="utf-16", sep=";")
# The data loads as one messy column, so we split it by comma
split_data = raw_vivno.iloc[:, 0].str.split(",", expand=True)
cols = [
    "Names",
    "color_wine",
    "Prices",
    "ML",
    "Ratings",
    "Ratingsnum",
    "Countrys",
    "ABV",
    "rates",
]
w = split_data.iloc[:, :9].copy()
w.columns = cols

# 2. winemag (Catalog B - Inference/Recommendation Target Content)
wm = pd.read_csv("winemag-data-130k-v2.csv", index_col=0)

# 3. created_wine_ratings (User Ratings Data)
ratings = pd.read_csv("created_wine_ratings.csv")

print("Datasets loaded:")
print(f"  - vivno (Catalog A): {w.shape}")
print(f"  - winemag (Catalog B): {wm.shape}")
print(f"  - ratings (User Data): {ratings.shape}")


# --- 1. Helper Function Definitions ---
# Define standardized helper functions that will be applied to BOTH
# 'vivno' and 'winemag' datasets to ensure their features are aligned.

# Map for standardizing country names
country_map = {
    "California": "US",
    "Napa": "US",
    "Sonoma": "US",
    "Oregon": "US",
    "Washington": "US",
    "Willamette": "US",
    "France": "France",
    "Bordeaux": "France",
    "Burgundy": "France",
    "Champagne": "France",
    "Rhône": "France",
    "Loire": "France",
    "Italy": "Italy",
    "Tuscany": "Italy",
    "Sicily": "Italy",
    "Piedmont": "Italy",
    "Spain": "Spain",
    "Rioja": "Spain",
    "Ribera": "Spain",
    "Germany": "Germany",
    "Australia": "Australia",
    "Portugal": "Portugal",
    "Argentina": "Argentina",
    "Chile": "Chile",
    "South Africa": "South Africa",
}


def extract_country(x):
    """Extracts a standard country name from a descriptive text string."""
    if pd.isna(x):
        return "US"  # Default to 'US' if missing
    for key, val in country_map.items():
        if key.lower() in x.lower():
            return val
    return "US"  # Default to 'US' if no keywords match


def price_level(p):
    """Converts a numeric price into three categories: 'low', 'medium', 'high'."""
    if pd.isna(p):
        return "medium"
    if p < 20:
        return "low"
    elif p < 50:
        return "medium"
    else:
        return "high"


def abv_level(a):
    """
    Converts numeric ABV to categories: 'low', 'medium', 'high'.

    [CRITICAL FIX]: This function handles the severe data quality issue in
    'vivno_dataset.csv' where missing ABV is marked as 0.0.
    We treat any value < 5% (including 0.0 and NaN) as 'medium' (unknown/average),
    not 'low'. This was a key finding from our EDA.
    """
    if pd.isna(a) or a < 5:  # Treat 0.0 and NaN as 'medium'
        return "medium"
    if a < 12:
        return "low"
    elif a < 14:
        return "medium"
    else:
        return "high"


def simplify_color(c):
    """Standardizes the 'color_wine' text into 4 main categories."""
    if pd.isna(c):
        return "Red Wine"
    if "Red" in c:
        return "Red Wine"
    if "White" in c:
        return "White Wine"
    if "Sparkling" in c or "Champagne" in c:
        return "Sparkling & Champagne"
    if "rose" in c or "rosé" in c or "pink" in c:
        return "Pink and Rosé"
    return "Red Wine"  # Default to most common category


print("Helper functions defined. (abv_level function includes critical 0.0 fix)")


# --- 2. (Part 1) Preprocess 'vivno_dataset.csv' (Catalog A) ---
# Prepare the training content data for modeling and for bridge creation.

print("\n[Part 1] 'vivno' (Catalog A) preprocessing...")

# 1. Type Conversion
# (Explanation: pd.to_numeric(errors='coerce') converts all values to numbers.
# Any values that fail conversion (like '$' or ',') become 'NaN' (Not a Number).)
w["Prices"] = (
    w["Prices"]
    .astype(str)
    .str.replace('"', "")
    .str.replace("$", "")
    .str.replace(",", "")
    .str.strip()
)
w["Prices"] = pd.to_numeric(w["Prices"], errors="coerce")
w["ABV"] = pd.to_numeric(w["ABV"], errors="coerce")
w["Ratings"] = pd.to_numeric(w["Ratings"], errors="coerce")
w["Ratingsnum"] = pd.to_numeric(w["Ratingsnum"], errors="coerce").fillna(0).astype(int)

# 2. Apply Helper Functions
w["price_level"] = w["Prices"].apply(price_level)
w["abv_level"] = w["ABV"].apply(abv_level)  # Our fix for 0.0 is applied here
w["country_norm"] = w["Countrys"].apply(extract_country)
w["color_simple"] = w["color_wine"].apply(simplify_color)

# 3. [KEY FEATURE] Extract 'variety' (Grape Type)
# This is the single most important feature, as it will act as the "Bridge"
# connecting 'vivno' to 'winemag'.
# We extract it from the 'Countrys' column (e.g., "Pinot Noir from Oregon").
w["variety"] = w["Countrys"].str.split(" from ").str[0].str.strip()
# Clean up cases where a country/region was mistaken for a variety
w["variety"] = w["variety"].apply(
    lambda x: x if x not in country_map.values() else np.nan
)
w["variety"] = w["variety"].apply(
    lambda x: x if x not in country_map.keys() else np.nan
)


# 4. Create the final clean DataFrame
cols_to_keep = [
    "Names",
    "variety",
    "country_norm",
    "color_simple",
    "price_level",
    "abv_level",
    "ABV",
    "Ratings",
    "Ratingsnum",
    "Prices",  # Include original 'Prices' and 'ABV' for EDA
]
w_clean = w[cols_to_keep].copy()

print("'vivno' preprocessing complete. 'variety' column created.")
print(w_clean.head())


# --- 3. (EDA 1) Explore 'vivno' (Catalog A) ---

print("\n[EDA 1] Exploring 'vivno' (Catalog A)...")

# Check for missing values in the original numeric columns
print(w[["Prices", "ABV", "Ratings", "Ratingsnum"]].isnull().sum())

fig, axes = plt.subplots(2, 2, figsize=(16, 10))
fig.suptitle("vivno (Catalog A) Feature Distributions", fontsize=16, y=1.02)

# 1. Wine Color Distribution (After processing)
sns.countplot(
    ax=axes[0, 0],
    data=w_clean,
    x="color_simple",
    order=w_clean["color_simple"].value_counts().index,
)
axes[0, 0].set_title("Wine Color Distribution (Processed)")

# 2. Price Distribution (Clamped at $200 to see detail)
sns.histplot(
    ax=axes[0, 1], data=w_clean[w_clean["Prices"] < 200], x="Prices", bins=50, kde=True
)
axes[0, 1].set_title("Price Distribution (< $200)")

# 3. ABV (Alcohol) Distribution
# [KEY FINDING]: The chart clearly shows a massive spike at 0.0,
# confirming our hypothesis that 0.0 is used for missing data.
# This proves that using mean() on this column would be incorrect.
sns.histplot(ax=axes[1, 0], data=w_clean, x="ABV", bins=30, kde=True)
axes[1, 0].set_title("ABV (Alcohol) Distribution")

# 4. Top 10 Countries (After processing)
top_10_countries = w_clean["country_norm"].value_counts().nlargest(10).index
sns.countplot(
    ax=axes[1, 1],
    data=w_clean[w_clean["country_norm"].isin(top_10_countries)],
    x="country_norm",
    order=top_10_countries,
)
axes[1, 1].set_title("Top 10 Countries (Processed)")

plt.tight_layout()
plt.show()


# --- 4. (Part 2) Create the "Bridge" (Mapping Tables) ---
# Create mapping tables (Python dictionaries) from 'vivno' data.
# These maps will be used to 'transfer' knowledge to the 'winemag' dataset.

print("\n[Part 2] Creating 'Bridge' (mapping tables)...")


# Helper function to get the mode (most frequent value)
def get_mode(x):
    """(Explanation: .mode() returns a Series, so we take the
    first item [0]. A try/except handles rare empty groups.)"""
    try:
        return x.mode()[0]
    except IndexError:
        # This specifically catches the error when the mode Series is empty.
        return np.nan


# 1. Bridge for Color: {Variety -> Most Common Color}
variety_to_color_map = (
    w_clean.dropna(subset=["variety", "color_simple"])
    .groupby("variety")["color_simple"]
    .apply(get_mode)
)

# 2. Bridge for ABV: {Variety -> Most Common ABV Level}
# [CRITICAL DECISION]: We map the *categorical* 'abv_level' (using .mode())
# instead of the *numeric* 'ABV' (using .mean()).
# This is because the 'ABV' column is unreliable (full of 0.0s),
# but our 'abv_level' column ('medium') is reliable.
variety_to_abv_level_map = (
    w_clean.dropna(subset=["variety", "abv_level"])
    .groupby("variety")["abv_level"]
    .apply(get_mode)
)


print(
    f"  - Color map created (e.g., 'Cabernet Sauvignon' -> '{variety_to_color_map.get('Cabernet Sauvignon')}')"
)
print(
    f"  - ABV Level map created (e.g., 'Cabernet Sauvignon' -> '{variety_to_abv_level_map.get('Cabernet Sauvignon')}')"
)
print("Bridge creation complete.")


# --- 5. (EDA 2) Explore 'winemag' (Catalog B) ---

print("\n[EDA 2] Exploring 'winemag' (Catalog B)...")

# Check missing values in key 'winemag' columns
print(wm[["price", "points"]].isnull().sum())

fig, axes = plt.subplots(2, 2, figsize=(16, 10))
fig.suptitle("winemag (Catalog B) Feature Distributions", fontsize=16, y=1.02)

# 1. WineEnthusiast Points Distribution
sns.histplot(ax=axes[0, 0], data=wm, x="points", bins=20, kde=False)
axes[0, 0].set_title("WineEnthusiast Points Distribution")

# 2. Price Distribution (Clamped at $200)
sns.histplot(ax=axes[0, 1], data=wm[wm["price"] < 200], x="price", bins=50, kde=True)
axes[0, 1].set_title("Price Distribution (< $200)")

# 3. Top 10 Countries (Raw data)
top_10_countries = wm["country"].value_counts().nlargest(10).index
sns.countplot(
    ax=axes[1, 0],
    data=wm[wm["country"].isin(top_10_countries)],
    x="country",
    order=top_10_countries,
)
axes[1, 0].set_title("Top 10 Countries (Raw)")
axes[1, 0].tick_params(axis="x", rotation=45)

# 4. Top 10 Varieties (Raw data)
# This 'variety' column is the key to our bridge.
top_10_varieties = wm["variety"].value_counts().nlargest(10).index
sns.countplot(
    ax=axes[1, 1],
    data=wm[wm["variety"].isin(top_10_varieties)],
    x="variety",
    order=top_10_varieties,
)
axes[1, 1].set_title("Top 10 Varieties (Raw)")
axes[1, 1].tick_params(axis="x", rotation=45)

plt.tight_layout()
plt.show()


# --- 6. (Part 3) Preprocess 'winemag-data-130k-v2.csv' (Catalog B) ---
# Standardize 'winemag' features to match 'vivno'.

print("\n[Part 3] 'winemag' (Catalog B) preprocessing...")

# 1. Apply the *same* helper functions used for 'vivno'
wm["country_norm"] = wm["country"].apply(extract_country)
wm["price_level"] = wm["price"].apply(price_level)

# 2. [KEY ACTION] Apply the "Bridge"
# (Explanation: .map() is a pandas function that applies a dictionary
# (our bridge) to a column ('variety'). If a variety is not in the map,
# it will result in 'NaN'.)
wm["color_simple"] = wm["variety"].map(variety_to_color_map)
wm["abv_level"] = wm["variety"].map(variety_to_abv_level_map)

# 3. Handle Missing Values (Post-Bridge)
# (Explanation: .fillna() is used to fill 'NaN' values.
# This handles new varieties in 'winemag' that were not in our 'vivno'
# bridge, ensuring no missing data in our final features.)
wm["color_simple"].fillna("Other", inplace=True)
wm["abv_level"].fillna("medium", inplace=True)  # Default for unknown varieties

# 5. Create the final clean 'winemag' DataFrame
wm_clean = wm[
    [
        "title",
        "variety",
        "country_norm",
        "color_simple",
        "price_level",
        "abv_level",
        "price",
        "points",
        "description",
    ]
].copy()

print("'winemag' preprocessing complete. 'color_simple' & 'abv_level' created.")
print(wm_clean.head())


# --- 7. (EDA 3) Explore 'created_wine_ratings' (User Data) ---

print("\n[EDA 3] Exploring 'created_wine_ratings' (User Data)...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle("created_wine_ratings (User Data) Distributions", fontsize=16, y=1.02)

# 1. Final Rating Distribution
# [KEY FINDING]: The ratings are heavily biased towards 4-stars.
# This is a critical insight, as it will bias our CBF model.
# This justifies using 'class_weight="balanced"' during modeling.
sns.countplot(ax=axes[0, 0], data=ratings, x="rating", palette="viridis")
axes[0, 0].set_title("Final Rating Distribution (Soft-Weighted)")

# 2. User Persona: Country Distribution
sns.countplot(
    ax=axes[0, 1],
    data=ratings,
    y="persona_country",
    order=ratings["persona_country"].value_counts().index,
    palette="crest",
)
axes[0, 1].set_title("User Persona: Country Distribution")

# 3. User Persona: Wine Color Distribution
sns.countplot(
    ax=axes[1, 0],
    data=ratings,
    x="persona_color",
    order=ratings["persona_color"].value_counts().index,
    palette="flare",
)
axes[1, 0].set_title("User Persona: Wine Color Distribution")
axes[1, 0].tick_params(axis="x", rotation=30)

# 4. User Persona: Price & ABV Level Distribution
# (Explanation: .twinx() creates a second y-axis to overlay two plots)
ax4_1 = axes[1, 1]
sns.countplot(
    ax=ax4_1,
    data=ratings,
    x="persona_price",
    order=["low", "medium", "high"],
    palette="mako",
    alpha=0.7,
    label="Price",
)
ax4_2 = ax4_1.twinx()
sns.countplot(
    ax=ax4_2,
    data=ratings,
    x="persona_abv",
    order=["low", "medium", "high"],
    palette="rocket",
    alpha=0.7,
    label="ABV",
)
ax4_1.set_title("User Persona: Price & ABV Level")
ax4_1.legend(loc="upper left")
ax4_2.legend(loc="upper right")

plt.tight_layout()
plt.show()


# --- 8. (Part 4) Create Final Training Data for CBF ---
# Merge user ratings with item features to create the final
# dataset for training the Content-Based Filtering (CBF) model.

print("\n[Part 4] Merging user ratings with 'vivno' content...")

# 1. CF Training Data (is just the 'ratings' DataFrame)
print(f"CF training data (ratings): {ratings.shape}")

# 2. CBF Training Data
# (Explanation: pd.merge links 'ratings' and 'w_clean' using the
# wine name ('item_id' and 'Names') as the common key.)
merged_train_data = pd.merge(
    ratings, w_clean, left_on="item_id", right_on="Names", how="left"
)
merged_train_data = merged_train_data.drop(columns=["Names"])

# [BUG FIX]: Removed an erroneous .dropna() line from a previous
# version. We need the *full* merged_train_data (including NaNs
# and persona columns) for the final modeling step.

print(f"\nCBF merged training data (merged_train_data): {merged_train_data.shape}")
print(merged_train_data.head())


# --- 9. (Part 5) Preprocess 'description' with TF-IDF (for Analysis) ---
# [Project Limitation]: This TF-IDF matrix CANNOT be used for the CBF
# model because the 'vivno' dataset does not have a 'description' column.
# We process it here only for potential future use or analysis.

print("\n[Part 5] 'winemag' description TF-IDF processing...")

# 1. Fill missing descriptions with empty strings
wm_clean["description"] = wm_clean["description"].fillna("")

# 2. Initialize TfidfVectorizer (See explanation in first cell)
tfidf_vec = TfidfVectorizer(stop_words="english", max_features=5000, ngram_range=(1, 2))

# 3. Fit and transform the descriptions into a sparse matrix
tfidf_matrix = tfidf_vec.fit_transform(wm_clean["description"])

# 4. Get the list of 5000 words/phrases (the feature names)
tfidf_features = tfidf_vec.get_feature_names_out()

print(f"TF-IDF matrix created: {tfidf_matrix.shape}")
print(f"Top 20 features (words): {tfidf_features[:20]}")


# --- 10. (Part 6) Save All Preprocessed Files ---
# Save all our hard work so the modeling notebook can load them easily.

print("\n[Part 6] Saving all preprocessed files...")

# 1. Preprocessed 'vivno' (Catalog A)
w_clean.to_csv("preprocessed_vivno.csv", index=False)
print(f"Saved: preprocessed_vivno.csv ({w_clean.shape})")

# 2. Preprocessed 'winemag' (Catalog B)
wm_clean.to_csv("preprocessed_winemag.csv", index=False)
print(f"Saved: preprocessed_winemag.csv ({wm_clean.shape})")

# 3. Final CBF Training Data (Merged)
# We save the full merged dataset created in Part 4.
merged_train_data.to_csv("preprocessed_merged_train.csv", index=False)
print(f"Saved: preprocessed_merged_train.csv ({merged_train_data.shape})")

# 4. TF-IDF Sparse Matrix
# (Explanation: save_npz is used for efficiently saving sparse matrices.)
save_npz("tfidf_matrix.npz", tfidf_matrix)
print(f"Saved: tfidf_matrix.npz ({tfidf_matrix.shape})")

# 5. TF-IDF Feature Name List
# (Explanation: pickle.dump saves the Python list of words.)
with open("tfidf_features.pkl", "wb") as f:
    pickle.dump(tfidf_features, f)
print(f"Saved: tfidf_features.pkl ({len(tfidf_features)} words)")

print("\n--- Preprocessing and file saving complete. ---")
